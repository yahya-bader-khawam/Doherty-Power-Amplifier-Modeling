# -*- coding: utf-8 -*-
"""Doherty Power Amplifiers Modeling Using Deep Convolutional Neural Networks and BiLSTM models.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1Bh0y00sOKrd_FOglw6MAGRuNlCv8pR6y
"""

import pandas as pd
import numpy as np
from keras.models import load_model
import tensorflow as tf
from keras.models import Sequential
from tensorflow import keras
import math as m
from numpy import zeros, newaxis
from keras.callbacks import TensorBoard
from livelossplot import PlotLossesKeras
from keras.layers import Dropout
import matplotlib.pyplot as plt
import scipy
from keras.layers.recurrent import LSTM
from keras.layers import Bidirectional, Conv2D, Flatten 
from keras.layers.convolutional import Convolution2D

from keras_radam import RAdam
from keras.optimizers import Adam, RMSprop, SGD
from keras.utils import plot_model
from keras.models import Model
from keras.layers import Input
from keras.layers import Dense
import keras.backend as K


# read input and output data. each file has to have two column one in-phase signal component, and the other one is quadrature component.
input_data_pd=pd.read_excel('input_data.xlsx')
output_data_pd=pd.read_excel('output_data.xlsx')

# convert dataframe of input and output to numpy array
input_data=input_data_pd.values
output_data=output_data_pd.values


def IQ_to_memory_matrix(input_data, output_data, start_indx, end_indx, m):
  '''converts in-phase and quadrature component signals to memory matrix to model the memory effect \
     of the Doherty Power Amplifier.

  - Arguments:
    * input_data: numpy array of the input in-phase and quadrature components.
    * output_data: numpy array of the output in-phase and quadrature components.
    * start_indx: index at which to start slicing intput and output arrays.
    * end_indx: index at which to end slicing intput and output arrays.
    * m: memory depth of the matrix
  
  - Returns:
    * seq_i: input in-phase and quadrature components with memory effect.
    * seq_o: output in-phase and quadrature components with memory effect.
    * input_dim: the dimension of the input matrix.
  '''
  # slice the in-phase and quadrature components from the input signal
  Ii=input_data[start_indx:end_indx,0]
  Qi=input_data[start_indx:end_indx,1]
  # slice the in-phase and quadrature components from the output signal
  Io=output_data[start_indx:end_indx,0]
  Qo=output_data[start_indx:end_indx,1]
  # convert the input and output signals to complex-valued signals 
  si=Ii+Qi*1j
  so=Io+Qo*1j

  # creating memory matrix
  seq_Ii=np.zeros([end_indx,m+1]) 
  seq_Qi=np.zeros([end_indx,m+1])
  for i in range(0,m+1):
      seq_Ii[:,-1-i]=np.concatenate([np.zeros(i),Ii[0:end_indx-i]])
      seq_Qi[:,-1-i]=np.concatenate([np.zeros(i),Qi[0:end_indx-i]])
      
  seq_Ii=np.flip(seq_Ii, axis=1)
  seq_Qi=np.flip(seq_Qi, axis=1)
  Xn=seq_Ii+seq_Qi*1j
  Xn1=abs(Xn)
  Xn2=abs(Xn)**2
  Xn3=abs(Xn)**3
  new_seq = []
  temp=[]
  for i in range(len(Ii)):
      temp = [seq_Ii[i,:],seq_Qi[i,:],Xn1[i,:],Xn2[i,:],Xn3[i,:]]
      new_seq.append(temp)
  seq_i=np.asarray(new_seq)
  # complex-valued input signals with memory 
  seq_i=seq_i[:,:,:,newaxis]
  input_dim = seq_i.shape[1:]
  # complex-valued output signals with memory 
  seq_o=np.concatenate([Io.reshape(len(Ii),1),Qo.reshape(len(Ii),1)],axis=1) 

  return seq_i, seq_o, input_dim


def Klog10(x):
  ''' computes the base-10 log of value x 
  '''
  numerator = K.log(x)
  denominator = K.log(tf.constant(10, dtype=numerator.dtype))
  return numerator / denominator


def custom_nmse(y_measure,y_sim):
  ''' computes the normalized mean square error between measured and simulated values.

  - Arguments:
    * y_measure: measured values
    * y_sim: simulated values

  - Returns:
    * e: normalized mean square error
  '''
  r1=K.sum(K.square(K.abs(y_measure-y_sim)))
  r2=K.sum(K.square(K.abs(y_measure)))
  e = 10*Klog10(r1/r2)
  return e

# create training dataset
seq_i, seq_o, input_dim = IQ_to_memory_matrix(input_data, output_data, 0, 40000, 6)
# create validation dataset
seq_i_val, seq_o_val, input_dim = IQ_to_memory_matrix(input_data, output_data, 40001, 80000, 6)



def CNN_model(seq_i, seq_o, seq_i_val, seq_o_val, input_dim):
  ''' Models the Doherty Power Amplifier using CNN model.

  - Arguments:
    * seq_i: input memory signals for model training.
    * seq_o: output memory signals for model training.
    * seq_i_val: input memory signals for model validation.
    * seq_o_val: output memory signals for model validation.
    * input_dim: the dimension of the input matrix.

  - Returns:
    * out: the output inferenced signal by the model.
    * history: history of model training.
    * model: the training model object.
  '''
  visible = Input(shape=input_dim)
  x = Conv2D(3, kernel_size=(3,3), strides=(1, 1), activation='tanh',)(visible)
  x = Flatten()(x)
  x = Dense(15, activation='tanh')(x)
  x = Dense(2,activation='tanh')(x)
  model = Model(inputs=visible,outputs=x)  
  print(model.summary())
  model.compile(optimizer=Adam(learning_rate=1e-4,  clipnorm=1.), 
                loss=custom_nmse, 
                metrics=['accuracy'])
  history = model.fit(seq_i,
                      seq_o, 
                      validation_data=(seq_i_val,seq_o_val),
                      batch_size=40000,
                      epochs=100000,
                      shuffle=False, 
                      verbose=1, 
                      callbacks=[])
  out = model.predict(seq_i)
  out = out[:,0]+out[:,1]*1j
  return out, history, model

def BiLSTM_model(seq_i, seq_o, seq_i_val, seq_o_val, input_dim):  
  ''' Models the Doherty Power Amplifier using BiLSTM model.

  - Arguments:
    * seq_i: input memory signals for model training.
    * seq_o: output memory signals for model training.
    * seq_i_val: input memory signals for model validation.
    * seq_o_val: output memory signals for model validation.
    * input_dim: the dimension of the input matrix.

  - Returns:
    * out: the output inferenced signal by the model.
    * history: history of model training.
    * model: the training model object.
  '''
  visible = Input(shape=input_dim)
  x = Bidirectional(LSTM((m+1),return_sequences=True, return_state=True, activation='tanh'))(visible)
  x = Bidirectional(LSTM((m+1), activation='tanh'))(x)
  x = Dense(20, activation='tanh')(x)
  x = Dense(2,activation='tanh')(x)
  model = Model(inputs=visible,outputs=x)  
  print(model.summary())
  model.compile(optimizer=Adam(learning_rate=0.5e-3,  clipnorm=1.), loss=custom_nmse, metrics=['accuracy'])
  history = model.fit(seq_i,seq_o, validation_data=(seq_i_val,seq_o_val), batch_size=10000, epochs=5000, shuffle=False, verbose=1, callbacks=[])
  out = model.predict(seq_i)
  out = out[:,0]+out[:,1]*1j
  return out, history, model

out, history, model = CNN_model(seq_i, seq_o, seq_i_val, seq_o_val, input_dim) # or use BiLSTM_model for deep BiLSTM model

